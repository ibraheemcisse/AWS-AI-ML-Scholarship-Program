# Enhanced Building Energy Efficiency Prediction with Random Forest
# This comprehensive implementation includes realistic data generation, 
# advanced model evaluation, hyperparameter tuning, and professional visualizations

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.model_selection import (train_test_split, cross_val_score, 
                                   GridSearchCV, validation_curve)
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (mean_squared_error, mean_absolute_error, 
                           r2_score, mean_absolute_percentage_error)
from sklearn.inspection import permutation_importance
import scipy.stats as stats

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class BuildingEnergyPredictor:
    """
    Enhanced Building Energy Efficiency Prediction System
    
    Features:
    - Realistic synthetic data generation based on building physics
    - Comprehensive model evaluation with multiple metrics
    - Hyperparameter tuning with cross-validation
    - Advanced visualizations and interpretability
    - Statistical analysis and model diagnostics
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        self.results = {}
        
    def generate_realistic_building_data(self, n_samples=1000):
        """
        Generate synthetic building data with realistic physical relationships
        """
        np.random.seed(self.random_state)
        
        # Primary building characteristics
        wall_area = np.random.normal(300, 50, n_samples)  # m²
        roof_area = np.random.normal(150, 30, n_samples)  # m²
        overall_height = np.random.uniform(2.5, 12, n_samples)  # meters
        glazing_ratio = np.random.beta(2, 3, n_samples)  # 0-1, skewed toward lower values
        
        # Additional realistic features
        insulation_thickness = np.random.normal(0.15, 0.05, n_samples)  # meters
        hvac_efficiency = np.random.normal(0.85, 0.1, n_samples)  # efficiency ratio
        building_age = np.random.exponential(15, n_samples)  # years
        orientation_factor = np.random.uniform(0.8, 1.2, n_samples)  # solar orientation
        
        # Derived features based on building physics
        total_envelope_area = wall_area + roof_area
        building_volume = wall_area * overall_height / 4  # Simplified volume
        surface_to_volume_ratio = total_envelope_area / building_volume
        glazing_area = wall_area * glazing_ratio
        
        # Generate energy efficiency based on realistic physical relationships
        base_efficiency = 25  # Base energy efficiency rating
        
        # Physical factors affecting energy efficiency
        envelope_loss = -0.02 * total_envelope_area  # Heat loss through envelope
        glazing_loss = -15 * glazing_ratio  # Heat loss through windows
        insulation_gain = 50 * insulation_thickness  # Insulation benefit
        hvac_gain = 20 * hvac_efficiency  # HVAC system efficiency
        age_penalty = -0.3 * building_age  # Degradation over time
        orientation_effect = 5 * (orientation_factor - 1)  # Solar orientation
        compactness_gain = -10 * surface_to_volume_ratio  # Building compactness
        
        # Interaction effects
        glazing_insulation_interaction = 5 * glazing_ratio * insulation_thickness
        
        energy_efficiency = (base_efficiency + envelope_loss + glazing_loss + 
                           insulation_gain + hvac_gain + age_penalty + 
                           orientation_effect + compactness_gain + 
                           glazing_insulation_interaction +
                           np.random.normal(0, 2, n_samples))  # Random noise
        
        # Ensure realistic bounds
        energy_efficiency = np.clip(energy_efficiency, 10, 50)
        insulation_thickness = np.clip(insulation_thickness, 0.05, 0.5)
        hvac_efficiency = np.clip(hvac_efficiency, 0.6, 1.0)
        building_age = np.clip(building_age, 0, 50)
        
        self.data = pd.DataFrame({
            'WallArea': wall_area,
            'RoofArea': roof_area,
            'OverallHeight': overall_height,
            'GlazingRatio': glazing_ratio,
            'InsulationThickness': insulation_thickness,
            'HVACEfficiency': hvac_efficiency,
            'BuildingAge': building_age,
            'OrientationFactor': orientation_factor,
            'TotalEnvelopeArea': total_envelope_area,
            'BuildingVolume': building_volume,
            'SurfaceToVolumeRatio': surface_to_volume_ratio,
            'GlazingArea': glazing_area,
            'EnergyEfficiency': energy_efficiency
        })
        
        return self.data
    
    def exploratory_data_analysis(self):
        """
        Comprehensive exploratory data analysis
        """
        print("=== DATASET OVERVIEW ===")
        print(f"Dataset shape: {self.data.shape}")
        print(f"\nDescriptive Statistics:")
        print(self.data.describe().round(2))
        
        print(f"\nMissing values:")
        print(self.data.isnull().sum())
        
        # Correlation analysis
        plt.figure(figsize=(15, 12))
        
        # Correlation heatmap
        plt.subplot(2, 3, 1)
        corr_matrix = self.data.corr()
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', 
                   center=0, square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
        plt.title('Feature Correlation Matrix')
        
        # Distribution of target variable
        plt.subplot(2, 3, 2)
        sns.histplot(data=self.data, x='EnergyEfficiency', kde=True, bins=30)
        plt.title('Energy Efficiency Distribution')
        plt.xlabel('Energy Efficiency Rating')
        
        # Key relationships
        plt.subplot(2, 3, 3)
        sns.scatterplot(data=self.data, x='GlazingRatio', y='EnergyEfficiency', alpha=0.6)
        plt.title('Glazing Ratio vs Energy Efficiency')
        
        plt.subplot(2, 3, 4)
        sns.scatterplot(data=self.data, x='InsulationThickness', y='EnergyEfficiency', alpha=0.6)
        plt.title('Insulation vs Energy Efficiency')
        
        plt.subplot(2, 3, 5)
        sns.scatterplot(data=self.data, x='SurfaceToVolumeRatio', y='EnergyEfficiency', alpha=0.6)
        plt.title('Surface-to-Volume Ratio vs Energy Efficiency')
        
        plt.subplot(2, 3, 6)
        sns.scatterplot(data=self.data, x='BuildingAge', y='EnergyEfficiency', alpha=0.6)
        plt.title('Building Age vs Energy Efficiency')
        
        plt.tight_layout()
        plt.show()
        
        # Statistical tests
        print(f"\n=== STATISTICAL TESTS ===")
        # Normality test for target variable
        stat, p_value = stats.shapiro(self.data['EnergyEfficiency'].sample(min(5000, len(self.data))))
        print(f"Shapiro-Wilk normality test p-value: {p_value:.6f}")
        print(f"Target distribution is {'normal' if p_value > 0.05 else 'not normal'}")
        
    def prepare_features(self):
        """
        Feature preparation and engineering
        """
        # Select features (excluding target and some derived features to avoid multicollinearity)
        feature_cols = ['WallArea', 'RoofArea', 'OverallHeight', 'GlazingRatio',
                       'InsulationThickness', 'HVACEfficiency', 'BuildingAge', 
                       'OrientationFactor', 'SurfaceToVolumeRatio']
        
        self.X = self.data[feature_cols]
        self.y = self.data['EnergyEfficiency']
        self.feature_names = feature_cols
        
        # Train-test split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=self.random_state, stratify=None
        )
        
        print(f"Training set size: {len(self.X_train)}")
        print(f"Test set size: {len(self.X_test)}")
        
    def train_and_tune_model(self):
        """
        Train Random Forest with hyperparameter tuning
        """
        print("=== MODEL TRAINING AND TUNING ===")
        
        # Define hyperparameter grid
        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None]
        }
        
        # Grid search with cross-validation
        rf = RandomForestRegressor(random_state=self.random_state, n_jobs=-1)
        
        print("Performing grid search with 5-fold cross-validation...")
        grid_search = GridSearchCV(
            rf, param_grid, cv=5, scoring='neg_mean_squared_error', 
            n_jobs=-1, verbose=1
        )
        
        grid_search.fit(self.X_train, self.y_train)
        
        self.model = grid_search.best_estimator_
        print(f"Best parameters: {grid_search.best_params_}")
        print(f"Best CV score (negative MSE): {grid_search.best_score_:.4f}")
        
        return grid_search
    
    def evaluate_model(self):
        """
        Comprehensive model evaluation
        """
        print("\n=== MODEL EVALUATION ===")
        
        # Predictions
        train_pred = self.model.predict(self.X_train)
        test_pred = self.model.predict(self.X_test)
        
        # Multiple evaluation metrics
        metrics = {}
        
        # Training metrics
        metrics['train_mse'] = mean_squared_error(self.y_train, train_pred)
        metrics['train_rmse'] = np.sqrt(metrics['train_mse'])
        metrics['train_mae'] = mean_absolute_error(self.y_train, train_pred)
        metrics['train_r2'] = r2_score(self.y_train, train_pred)
        metrics['train_mape'] = mean_absolute_percentage_error(self.y_train, train_pred)
        
        # Test metrics
        metrics['test_mse'] = mean_squared_error(self.y_test, test_pred)
        metrics['test_rmse'] = np.sqrt(metrics['test_mse'])
        metrics['test_mae'] = mean_absolute_error(self.y_test, test_pred)
        metrics['test_r2'] = r2_score(self.y_test, test_pred)
        metrics['test_mape'] = mean_absolute_percentage_error(self.y_test, test_pred)
        
        self.results = metrics
        
        # Display results
        print(f"Training Metrics:")
        print(f"  RMSE: {metrics['train_rmse']:.4f}")
        print(f"  MAE:  {metrics['train_mae']:.4f}")
        print(f"  R²:   {metrics['train_r2']:.4f}")
        print(f"  MAPE: {metrics['train_mape']:.4f}")
        
        print(f"\nTest Metrics:")
        print(f"  RMSE: {metrics['test_rmse']:.4f}")
        print(f"  MAE:  {metrics['test_mae']:.4f}")
        print(f"  R²:   {metrics['test_r2']:.4f}")
        print(f"  MAPE: {metrics['test_mape']:.4f}")
        
        # Cross-validation scores
        cv_scores = cross_val_score(self.model, self.X_train, self.y_train, 
                                   cv=5, scoring='r2')
        print(f"\nCross-validation R² scores: {cv_scores}")
        print(f"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        return test_pred
    
    def create_visualizations(self, test_pred):
        """
        Create comprehensive visualizations
        """
        plt.figure(figsize=(20, 15))
        
        # 1. True vs Predicted scatter plot
        plt.subplot(3, 4, 1)
        plt.scatter(self.y_test, test_pred, alpha=0.6, s=50)
        plt.plot([self.y_test.min(), self.y_test.max()], 
                [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        plt.xlabel('True Values')
        plt.ylabel('Predicted Values')
        plt.title('True vs Predicted Values')
        plt.text(0.05, 0.95, f'R² = {self.results["test_r2"]:.3f}', 
                transform=plt.gca().transAxes, bbox=dict(boxstyle="round", facecolor='wheat'))
        
        # 2. Residuals plot
        plt.subplot(3, 4, 2)
        residuals = self.y_test - test_pred
        plt.scatter(test_pred, residuals, alpha=0.6)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicted Values')
        plt.ylabel('Residuals')
        plt.title('Residual Plot')
        
        # 3. Residuals histogram
        plt.subplot(3, 4, 3)
        sns.histplot(residuals, kde=True, bins=30)
        plt.xlabel('Residuals')
        plt.title('Residuals Distribution')
        
        # 4. Feature importance
        plt.subplot(3, 4, 4)
        importances = self.model.feature_importances_
        indices = np.argsort(importances)[::-1]
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(range(len(importances)), 
                  [self.feature_names[i] for i in indices], rotation=45)
        plt.title('Feature Importances')
        plt.ylabel('Importance')
        
        # 5. Permutation importance
        plt.subplot(3, 4, 5)
        perm_importance = permutation_importance(
            self.model, self.X_test, self.y_test, 
            n_repeats=10, random_state=self.random_state
        )
        indices = np.argsort(perm_importance.importances_mean)[::-1]
        plt.bar(range(len(perm_importance.importances_mean)), 
                perm_importance.importances_mean[indices])
        plt.xticks(range(len(perm_importance.importances_mean)), 
                  [self.feature_names[i] for i in indices], rotation=45)
        plt.title('Permutation Importance')
        plt.ylabel('Importance')
        
        # 6. Prediction error distribution
        plt.subplot(3, 4, 6)
        errors = np.abs(self.y_test - test_pred)
        sns.histplot(errors, kde=True, bins=30)
        plt.xlabel('Absolute Error')
        plt.title('Prediction Error Distribution')
        
        # 7. Learning curves
        plt.subplot(3, 4, 7)
        train_sizes = np.linspace(0.1, 1.0, 10)
        train_scores, val_scores = validation_curve(
            self.model, self.X_train, self.y_train, 
            param_name='n_estimators', param_range=[50, 100, 150, 200, 250],
            cv=3, scoring='r2'
        )
        plt.plot([50, 100, 150, 200, 250], train_scores.mean(axis=1), 'o-', label='Training')
        plt.plot([50, 100, 150, 200, 250], val_scores.mean(axis=1), 'o-', label='Validation')
        plt.xlabel('n_estimators')
        plt.ylabel('R² Score')
        plt.title('Validation Curve')
        plt.legend()
        
        # 8. Most important features relationships
        most_important_idx = np.argsort(importances)[-3:]
        for i, idx in enumerate(most_important_idx):
            plt.subplot(3, 4, 8 + i)
            feature_name = self.feature_names[idx]
            plt.scatter(self.X_test.iloc[:, idx], self.y_test, alpha=0.6, label='True')
            plt.scatter(self.X_test.iloc[:, idx], test_pred, alpha=0.6, label='Predicted')
            plt.xlabel(feature_name)
            plt.ylabel('Energy Efficiency')
            plt.title(f'{feature_name} vs Target')
            plt.legend()
        
        # Q-Q plot for residuals normality
        plt.subplot(3, 4, 11)
        stats.probplot(residuals, dist="norm", plot=plt)
        plt.title('Q-Q Plot of Residuals')
        
        # Model complexity vs performance
        plt.subplot(3, 4, 12)
        max_depths = [5, 10, 15, 20, 25, None]
        train_scores = []
        val_scores = []
        
        for depth in max_depths:
            if depth is None:
                depth_val = 30  # For plotting
            else:
                depth_val = depth
            
            temp_model = RandomForestRegressor(
                n_estimators=100, max_depth=depth, random_state=self.random_state
            )
            temp_model.fit(self.X_train, self.y_train)
            
            train_pred_temp = temp_model.predict(self.X_train)
            val_pred_temp = temp_model.predict(self.X_test)
            
            train_scores.append(r2_score(self.y_train, train_pred_temp))
            val_scores.append(r2_score(self.y_test, val_pred_temp))
        
        x_vals = [5, 10, 15, 20, 25, 30]  # 30 represents None
        plt.plot(x_vals, train_scores, 'o-', label='Training R²')
        plt.plot(x_vals, val_scores, 'o-', label='Validation R²')
        plt.xlabel('Max Depth')
        plt.ylabel('R² Score')
        plt.title('Model Complexity vs Performance')
        plt.legend()
        
        plt.tight_layout()
        plt.show()
    
    def feature_analysis(self):
        """
        Detailed feature analysis and interpretation
        """
        print("\n=== FEATURE ANALYSIS ===")
        
        # Feature importance ranking
        importances = self.model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'Feature': self.feature_names,
            'Importance': importances
        }).sort_values('Importance', ascending=False)
        
        print("Feature Importance Ranking:")
        for idx, row in feature_importance_df.iterrows():
            print(f"  {row['Feature']}: {row['Importance']:.4f}")
        
        # Partial dependence analysis (simplified version)
        print(f"\nTop 3 Most Important Features Analysis:")
        top_features = feature_importance_df.head(3)['Feature'].values
        
        plt.figure(figsize=(15, 5))
        
        for i, feature in enumerate(top_features):
            plt.subplot(1, 3, i + 1)
            
            feature_idx = self.feature_names.index(feature)
            feature_values = self.X_test.iloc[:, feature_idx]
            
            # Create bins for the feature
            n_bins = 20
            bins = np.linspace(feature_values.min(), feature_values.max(), n_bins)
            bin_centers = (bins[:-1] + bins[1:]) / 2
            bin_predictions = []
            
            # Calculate average prediction for each bin
            for j in range(len(bins) - 1):
                mask = (feature_values >= bins[j]) & (feature_values < bins[j + 1])
                if mask.sum() > 0:
                    avg_pred = self.model.predict(self.X_test[mask]).mean()
                    bin_predictions.append(avg_pred)
                else:
                    bin_predictions.append(np.nan)
            
            # Remove NaN values
            valid_idx = ~np.isnan(bin_predictions)
            bin_centers = bin_centers[valid_idx]
            bin_predictions = np.array(bin_predictions)[valid_idx]
            
            plt.plot(bin_centers, bin_predictions, 'bo-')
            plt.xlabel(feature)
            plt.ylabel('Predicted Energy Efficiency')
            plt.title(f'Partial Dependence: {feature}')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def generate_predictions_report(self, sample_buildings=None):
        """
        Generate predictions for new buildings with confidence intervals
        """
        if sample_buildings is None:
            # Create some example buildings
            sample_buildings = pd.DataFrame({
                'WallArea': [250, 350, 400],
                'RoofArea': [120, 180, 200],
                'OverallHeight': [3.0, 8.0, 10.0],
                'GlazingRatio': [0.2, 0.4, 0.6],
                'InsulationThickness': [0.10, 0.20, 0.30],
                'HVACEfficiency': [0.75, 0.85, 0.95],
                'BuildingAge': [5, 15, 25],
                'OrientationFactor': [0.9, 1.0, 1.1],
                'SurfaceToVolumeRatio': [1.5, 1.2, 1.0]
            })
        
        print("\n=== SAMPLE PREDICTIONS ===")
        predictions = self.model.predict(sample_buildings)
        
        # Calculate prediction intervals using bootstrap
        n_bootstrap = 100
        bootstrap_preds = []
        
        for _ in range(n_bootstrap):
            # Bootstrap sample
            indices = np.random.choice(len(self.X_train), len(self.X_train), replace=True)
            X_bootstrap = self.X_train.iloc[indices]
            y_bootstrap = self.y_train.iloc[indices]
            
            # Train model on bootstrap sample
            bootstrap_model = RandomForestRegressor(
                **self.model.get_params(), random_state=None
            )
            bootstrap_model.fit(X_bootstrap, y_bootstrap)
            
            # Predict
            bootstrap_pred = bootstrap_model.predict(sample_buildings)
            bootstrap_preds.append(bootstrap_pred)
        
        bootstrap_preds = np.array(bootstrap_preds)
        pred_intervals = np.percentile(bootstrap_preds, [2.5, 97.5], axis=0)
        
        results_df = sample_buildings.copy()
        results_df['Predicted_Efficiency'] = predictions
        results_df['Lower_CI'] = pred_intervals[0]
        results_df['Upper_CI'] = pred_intervals[1]
        
        print(results_df.round(2))
        
        return results_df
    
    def run_complete_analysis(self, n_samples=1000):
        """
        Run the complete analysis pipeline
        """
        print("🏠 ENHANCED BUILDING ENERGY EFFICIENCY PREDICTION 🏠")
        print("=" * 60)
        
        # Generate data
        print("📊 Generating realistic building dataset...")
        self.generate_realistic_building_data(n_samples)
        
        # EDA
        print("\n🔍 Performing exploratory data analysis...")
        self.exploratory_data_analysis()
        
        # Feature preparation
        print("\n⚙️ Preparing features...")
        self.prepare_features()
        
        # Model training and tuning
        print("\n🤖 Training and tuning model...")
        grid_search = self.train_and_tune_model()
        
        # Model evaluation
        print("\n📈 Evaluating model performance...")
        test_pred = self.evaluate_model()
        
        # Visualizations
        print("\n📊 Creating comprehensive visualizations...")
        self.create_visualizations(test_pred)
        
        # Feature analysis
        print("\n🔬 Analyzing feature importance...")
        self.feature_analysis()
        
        # Sample predictions
        print("\n🏗️ Generating sample predictions...")
        sample_results = self.generate_predictions_report()
        
        print(f"\n✅ Analysis complete!")
        print(f"Final model performance: R² = {self.results['test_r2']:.4f}, RMSE = {self.results['test_rmse']:.4f}")
        
        return self.model, self.results

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    # Initialize and run the enhanced analysis
    predictor = BuildingEnergyPredictor(random_state=42)
    model, results = predictor.run_complete_analysis(n_samples=1000)
    
    print("\n" + "="*60)
    print("🎉 ENHANCED BUILDING ENERGY EFFICIENCY PREDICTION COMPLETE! 🎉")
    print("="*60)
